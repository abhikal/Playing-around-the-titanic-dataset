# -*- coding: utf-8 -*-
"""LOGISTIC TITANIC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mpx73UUI3hjbeYIq73U5OOjiROtkXuu9

**Importing the modules**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import statsmodels.api as sm

sns.set()

"""***Reading the raw file***"""

raw_data =pd.read_csv("train.csv")
raw_data.head()

"""***To see what all of it contains***"""

raw_data.describe(include='all')

"""**Dropping the irrevant coloumns. **
Note: Below i had seen how many null values each has and found feature 'Age' to have around 687 null values so i dropped it here only.
"""

new_data = raw_data.drop(['Name','Ticket','Cabin','Age'],axis=1)
new_data.describe(include='all')

"""***To check the null values:***"""

new_data.isnull().sum()

"""***Dropping the null values***"""

data = new_data.dropna(axis=0)
data.isnull().sum()

data.describe(include= 'all')

"""***Plotting various features against the independent feature:***
Note: Does not include the dummy variables
"""

sns.distplot(data['Pclass'])

sns.distplot(data['SibSp'])

"""As we can see this is not normally distributed and has outliers, so  trying to bring it to normal."""

q=data['SibSp'].quantile(0.99)
data1=data[data['SibSp']<q]
data1.describe(include='all')

sns.distplot(data['Parch'])

"""Above plot is a very disturbed plot, so have kept it for further improvement."""

sns.distplot(data['Fare'])

q=data['Fare'].quantile(0.99)
data1=data[data['Fare']<q]
data1.describe(include='all')

sns.distplot(data['Fare'])

data2=data1[data1['Parch']<5]
sns.distplot(data['Parch'])

"""Whatever is cleaned is included as the new data."""

data_cleaned=data2.reset_index(drop=True)
data_cleaned.describe()

"""***Making the final plots***"""

f,  (ax1,ax2,ax4,ax6) = plt.subplots(1,4, sharey= True, figsize=(15,3))
ax1.scatter(data_cleaned['Parch'],data_cleaned['Survived'])
ax1.set_title('Survived and Parch')
ax2.scatter(data_cleaned['Pclass'],data_cleaned['Survived'])
ax2.set_title('Survived and Pclass')
ax4.scatter(data_cleaned['SibSp'],data_cleaned['Survived'])
ax4.set_title('Survived' and 'SibSp')
#ax5.scatter(data_cleaned['PassengerID'],data_cleaned['Survived'])
#ax5.set_title['Survived' and 'Parch']
ax6.scatter(data_cleaned['Fare'],data_cleaned['Survived'])
ax6.set_title('Survived' and 'Fare')
plt.show()

data_cleaned.describe(include='all')

"""***Time for making the categorical variables the dummies:***"""

data_with_dummies = pd.get_dummies(data_cleaned)
data_with_dummies.head()

data_with_dummies.columns.values

"""***Final plotting with all the variables:***"""

y = data_with_dummies['Survived']
x1 = data_with_dummies[['PassengerId', 'Pclass', 'SibSp', 'Parch',
       'Fare', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q',
       'Embarked_S']]

"""***Fitting into the regression model:***"""

x = sm.add_constant(x1)
reg_log = sm.Logit(y,x)
results_log = reg_log.fit()
results_log.summary()

results_log.predict()

"""***Checking Predictions:***"""

np.array(data['Survived'])

results_log.pred_table()

"""***Getting the confusion Matrix:***"""

cm_df = pd.DataFrame(results_log.pred_table())
cm_df.columns = { 'Predicted 0', 'Predicted 1'}
cm_df = cm_df.rename(index={0:'Actual 0', 1:'Actual 1'})
cm_df

"""***Checking the accuracy:***"""

cm = np.array(cm_df)
accuracy_train = (cm[0,0]+cm[1,1])/cm.sum()
accuracy_train

"""Another method of trying Logistic Regression:"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
lr = LogisticRegression(max_iter = 2000)
cv = cross_val_score(lr,x,y,cv=5)
print(cv)
print(cv.mean())

"""Using decision tree:"""

from sklearn import tree
dt = tree.DecisionTreeClassifier(random_state = 1)
cv = cross_val_score(dt,x,y,cv=5)
print(cv)
print(cv.mean())

"""Using a Gaussian approach:"""

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
cv = cross_val_score(gnb,x,y,cv=5)
print(cv)
print(cv.mean())

"""K Neighbors:"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
cv = cross_val_score(knn,x,y,cv=5)
print(cv)
print(cv.mean())

"""Using SVC"""

from sklearn.svm import SVC
svc = SVC(probability = True)
cv = cross_val_score(svc,x,y,cv=5)
print(cv)
print(cv.mean())

"""Using Random Forest"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state = 1)
cv = cross_val_score(rf,x,y,cv=5)
print(cv)
print(cv.mean())

"""Note: For some Classifiers the accuracy is extremely low as because scaling is not done.
Choosed not to scale because of the presence of a large number of dummies.
"""